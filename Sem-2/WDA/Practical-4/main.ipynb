{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da15de1a",
   "metadata": {},
   "source": [
    "Practical-4: Demonstrate Text Mining and Webpage Pre-processing using meta information\n",
    "from the web pages (Local/Online)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6408aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\subhashish\\msc-cookbook\\sem-2\\wda\\.venv\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\subhashish\\msc-cookbook\\sem-2\\wda\\.venv\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\subhashish\\msc-cookbook\\sem-2\\wda\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\subhashish\\msc-cookbook\\sem-2\\wda\\.venv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\subhashish\\msc-cookbook\\sem-2\\wda\\.venv\\lib\\site-packages (from requests) (2025.7.9)\n",
      "Collecting bs4\n",
      "  Obtaining dependency information for bs4 from https://files.pythonhosted.org/packages/51/bb/bf7aab772a159614954d84aa832c129624ba6c32faa559dfb200a534e50b/bs4-0.0.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\subhashish\\msc-cookbook\\sem-2\\wda\\.venv\\lib\\site-packages (from bs4) (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\subhashish\\msc-cookbook\\sem-2\\wda\\.venv\\lib\\site-packages (from beautifulsoup4->bs4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\subhashish\\msc-cookbook\\sem-2\\wda\\.venv\\lib\\site-packages (from beautifulsoup4->bs4) (4.14.1)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install requests bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1848fbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\subha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\subha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\subha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\subha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Nextgenpixel - Get the Perfect Custom Design\n",
      "Description: Nextgenpixel is your one stop solution if you want to grow online because we offer the three core services that will essentially kickstart your business and take it to the next level.\n",
      "Keywords: No keywords\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set the target URL\n",
    "url = 'https://www.nextgenpixel.co.in/'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract the title\n",
    "    title = soup.title.string if soup.title else 'No title'\n",
    "\n",
    "    # Extract meta description\n",
    "    description_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "    description = description_tag['content'] if description_tag and 'content' in description_tag.attrs else 'No description'\n",
    "\n",
    "    # Extract meta keywords\n",
    "    keywords_tag = soup.find('meta', attrs={'name': 'keywords'})\n",
    "    keywords = keywords_tag['content'] if keywords_tag and 'content' in keywords_tag.attrs else 'No keywords'\n",
    "\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Keywords: {keywords}\")\n",
    "\n",
    "    # Combine the title, description, and keywords into a single text\n",
    "    text = f\"{title} {description} {keywords}\"\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Convert to lower case\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into a string\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    print(f\"Processed Text: {processed_text}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve webpage. Status code: {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
